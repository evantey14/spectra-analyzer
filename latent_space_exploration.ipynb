{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tftables\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import data_loader\n",
    "import model_short as model\n",
    "import toy_data_loader\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hps:\n",
    "    pass\n",
    "hps.n_levels = 3 # number of splits\n",
    "hps.depth = 3 # number of layers in revnet\n",
    "hps.width = 16 # channels in revnet layers\n",
    "hps.polyak_epochs = 1\n",
    "hps.beta1 = .9 # learning rate annealing factor\n",
    "hps.weight_decay = 1 # learning rate annealing factor\n",
    "hps.lr = .001 # base learning rate\n",
    "hps.n_data = 16000 # number of input spectra\n",
    "hps.batch_size = 50 # number of spectra in a batch\n",
    "hps.n_batches = int(hps.n_data / hps.batch_size)\n",
    "hps.n_bins = 2**12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select real or toy data by uncommenting the appropriate line\n",
    "# real data must have n_data=8000, n_bins=40000\n",
    "#input_stream, initialize_input_stream, data_init = data_loader.create_data_loader(\n",
    "input_stream, initialize_input_stream, data_init = toy_data_loader.create_data_loader(\n",
    "    sess, hps.batch_size, hps.n_data, hps.n_bins\n",
    ")\n",
    "'''\n",
    "spectra = np.load('sample_short.npz')['spectra']\n",
    "sqrt = np.sqrt(spectra)\n",
    "\n",
    "# add noise\n",
    "#sums = spectra.sum(axis=1)\n",
    "#sqrtsums = sqrt.sum(axis=1)\n",
    "#As = .02 * sums / (np.sqrt(2 / 3.14) * sqrtsums)\n",
    "#noise = np.random.normal(scale=(np.repeat(As[:, np.newaxis], hps.n_bins, axis=1) * sqrt))\n",
    "#print((np.abs(noise).sum(axis=1) / spectra.sum(axis=1)))\n",
    "\n",
    "scaled_spectra = spectra / spectra.std(axis=1)[:, np.newaxis]\n",
    "#scaled_spectra = (spectra + noise) / (spectra + noise).std(axis=1)[:, np.newaxis]\n",
    "centered_spectra = scaled_spectra - scaled_spectra.mean(axis=1)[:, np.newaxis]\n",
    "#normalized_spectra = spectra / np.max(spectra, axis=1)[:, np.newaxis]\n",
    "\n",
    "def create_data_loader(sess, data, batch_size):\n",
    "    placeholder_data = tf.compat.v1.placeholder(tf.float32, data.shape)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(placeholder_data)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    input_stream = iterator.get_next()\n",
    "    \n",
    "    def initialize_input_stream():\n",
    "        sess.run(iterator.initializer, feed_dict={placeholder_data: data})\n",
    "    \n",
    "    initialize_input_stream()\n",
    "    data_init = sess.run(input_stream)\n",
    "    return input_stream, initialize_input_stream, data_init\n",
    "\n",
    "input_stream, initialize_input_stream, data_init = create_data_loader(\n",
    "    sess, centered_spectra[:, :, np.newaxis], hps.batch_size\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data_init.shape)\n",
    "plt.figure(figsize=(15, 5))\n",
    "for spectrum in data_init[:25]:\n",
    "    plt.plot(spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/device:GPU:0\"):\n",
    "    m = model.model(sess, hps, input_stream, data_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.restore('models/model-200318-095826')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = np.random.randint(0, hps.batch_size)\n",
    "spectrum = data_init[i:i+1, :, :]\n",
    "latent_rep = m.encode(spectrum)\n",
    "reconstruction = m.decode(latent_rep)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = (1850, 2200) #(12000, 14000)\n",
    "window_size = (window[1] - window[0])\n",
    "window_fraction = window_size / hps.n_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(np.squeeze(reconstruction))\n",
    "plt.plot(np.squeeze(spectrum))\n",
    "plt.axvline(window[0])\n",
    "plt.axvline(window[1])\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(*window), np.squeeze(reconstruction)[window[0]:window[1]])\n",
    "plt.plot(range(*window), np.squeeze(spectrum)[window[0]:window[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_kernel(size, mean, std):\n",
    "    d = tf.distributions.Normal(tf.cast(mean, tf.float32), tf.cast(std, tf.float32))\n",
    "    vals = d.prob(tf.range(start=-int(size/2), limit=int(size/2)+1, dtype=tf.float32))\n",
    "\n",
    "    kernel = vals[:, np.newaxis, np.newaxis]\n",
    "    return kernel / tf.reduce_sum(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_kernel = create_gaussian_kernel(51, 0, 25)\n",
    "derivative_kernel = tf.constant([[[-hps.n_bins / 2]], [[0]], [[hps.n_bins / 2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed = tf.nn.conv1d(m.decoded_spectra, gaussian_kernel, padding=\"SAME\")\n",
    "first_derivative = tf.nn.conv1d(smoothed, derivative_kernel, padding=\"SAME\")\n",
    "smoothed_first_derivative = tf.nn.conv1d(first_derivative, gaussian_kernel, padding=\"SAME\")\n",
    "second_derivative = tf.nn.conv1d(smoothed_first_derivative, derivative_kernel, padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_spectra = sess.run(smoothed, {m.z_placeholder: latent_rep})\n",
    "first_derivative_spectra = sess.run(first_derivative, {m.z_placeholder: latent_rep})\n",
    "second_derivative_spectra = sess.run(second_derivative, {m.z_placeholder: latent_rep})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(np.squeeze(reconstruction))\n",
    "plt.plot(np.squeeze(smoothed_spectra))\n",
    "plt.plot(np.squeeze(first_derivative_spectra) / first_derivative_spectra.std())\n",
    "plt.plot(np.squeeze(second_derivative_spectra) / second_derivative_spectra.std())\n",
    "plt.ylim(-5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outside window\n",
    "left_squared_error = tf.reduce_sum((spectrum[:, :window[0]] - m.decoded_spectra[:, :window[0]])**2)\n",
    "right_squared_error = tf.reduce_sum((spectrum[:, window[1]:] - m.decoded_spectra[:, window[1]:])**2)\n",
    "outside_cost = left_squared_error + right_squared_error\n",
    "\n",
    "# inside window\n",
    "#inside_cost = tf.reduce_sum((spectrum[:, window[0]:window[1]] - m.decoded_spectra[:, window[0]:window[1]])**2)\n",
    "inside_cost = -tf.reduce_sum(second_derivative[:, window[0]:window[1]]**2)\n",
    "\n",
    "# likelihood\n",
    "logp = -.5 * np.sum(m.z_placeholder**2)\n",
    "beta = 1e11\n",
    "\n",
    "cost = inside_cost - outside_cost + beta * logp # maximize inside cost and likelihood. minimize outside cost\n",
    "gradient = tf.gradients(cost, m.z_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = []\n",
    "latent_reps = [latent_rep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    grads.append(sess.run(gradient, {m.z_placeholder: latent_reps[-1]})[0])\n",
    "    step_size = .01 / np.linalg.norm(grads[-1][0])\n",
    "    latent_reps.append(latent_reps[-1] + step_size * grads[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploration analysis\n",
    "latent_reps_np = np.array(latent_reps)\n",
    "grads_np = np.array(grads)\n",
    "print_freq = int(len(grads) / 20) # when plotting changes over time, plot around 20 things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(3, 2, 1)\n",
    "for i in [0, 1, 2, 3]:\n",
    "    plt.plot(latent_reps_np.mean(axis=(0, 1))[:, i])\n",
    "plt.xlabel('component position')\n",
    "plt.ylabel('latent rep (avg over steps)')\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "for i in [0, 1, 2, 3]:\n",
    "    plt.plot(grads_np.mean(axis=(0, 1))[:, i])\n",
    "plt.xlabel('component position')\n",
    "plt.ylabel('gradient (avg over steps)')\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot([np.linalg.norm(l) for l in latent_reps])\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('norm of latent representation')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot([np.linalg.norm(g) for g in grads])\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('norm of gradient')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "for i in range(0, len(latent_reps), print_freq):\n",
    "    plt.plot(latent_reps_np[i].sum(axis=(0, 2)))\n",
    "plt.xlim(100, 150)\n",
    "plt.xlabel('component position')\n",
    "plt.ylabel('latent rep (sum over channels) over time')\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "for i in range(0, len(grads), print_freq):\n",
    "    plt.plot(grads_np[i].sum(axis=(0, 2)))\n",
    "plt.xlim(100, 150)\n",
    "plt.xlabel('component position')\n",
    "plt.ylabel('gradient (sum over channels) over time')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(np.squeeze(reconstruction))\n",
    "for i in range(0, len(latent_reps), print_freq):\n",
    "    plt.plot(np.squeeze(m.decode(latent_reps[i])))\n",
    "plt.axvline(window[0])\n",
    "plt.axvline(window[1])\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(*window), np.squeeze(reconstruction)[window[0]:window[1]])\n",
    "for i in range(0, len(latent_reps), print_freq):\n",
    "    plt.plot(range(*window), np.squeeze(m.decode(latent_reps[i]))[window[0]:window[1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
