{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from data import data_loader, toy_data\n",
    "import glow as model\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick', labelsize=8)\n",
    "plt.rc('ytick', labelsize=8)\n",
    "plt.rc('axes', labelsize=8)\n",
    "plt.rc('figure', autolayout=True, dpi=300)\n",
    "plt.rc('lines', linewidth=1)\n",
    "plt.rc('legend', fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hps:\n",
    "    pass\n",
    "hps.n_levels = 10 # number of splits\n",
    "hps.depth = 3 # number of layers in revnet\n",
    "hps.width = 16 # channels in revnet layers\n",
    "hps.polyak_epochs = 1\n",
    "hps.beta1 = .9 # learning rate annealing factor\n",
    "hps.weight_decay = 1 # learning rate annealing factor\n",
    "hps.lr = .001 # base learning rate\n",
    "hps.n_data = 4000 # number of input spectra\n",
    "hps.batch_size = 50 # number of spectra in a batch\n",
    "hps.n_batches = int(hps.n_data / hps.batch_size)\n",
    "hps.n_bins = 2**12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.InteractiveSession()\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = 'toy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'toy':\n",
    "    spectra, labels = toy_data.generate_spectra(hps.n_data, hps.n_bins)\n",
    "    labels = labels[:, 1:3] # ignore temperature and sigma (leaving A, mu)\n",
    "    input_stream, label_stream, initialize_stream = (\n",
    "        data_loader.create_loader_from_array(sess, hps.batch_size, spectra, labels)\n",
    "    )\n",
    "elif data_source == 'real':\n",
    "    input_stream, label_stream, initialize_stream = (\n",
    "        data_loader.create_loader_from_hdf5(sess, hps.batch_size, 'data/sample_short.h5')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_stream()\n",
    "spectra = sess.run(input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "for spectrum in spectra[:5]:\n",
    "    plt.plot(spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with tf.device(\"/device:GPU:0\"):\n",
    "m = model.model(sess, hps, input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_processed = 0\n",
    "training_results = []\n",
    "lrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hps.epochs = 50\n",
    "hps.epochs_warmup = .01\n",
    "hps.lr = .001\n",
    "\n",
    "for epoch in tqdm(range(1, hps.epochs + 1), desc='Epochs'):\n",
    "    epoch_results = []\n",
    "    initialize_stream()\n",
    "    with tqdm(total=hps.n_batches) as pbar:\n",
    "        for iteration in range(hps.n_batches):\n",
    "            pbar.set_description('Epoch ' + str(epoch))\n",
    "            lr = hps.lr * min(1., n_processed / (hps.batch_size * hps.n_batches * hps.epochs_warmup))\n",
    "            training_result = [m.train(lr)]\n",
    "            epoch_results += training_result\n",
    "            training_results += training_result\n",
    "            lrs += [lr]\n",
    "            n_processed += hps.batch_size\n",
    "            pbar.set_postfix(lr=lr, loss=np.mean(epoch_results))\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(np.linspace(0, len(training_results) / hps.n_batches, len(training_results)), training_results)\n",
    "#plt.yscale('symlog')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(np.linspace(0, len(training_results) / hps.n_batches, len(training_results)), lrs)\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 2))\n",
    "\n",
    "plt.plot(np.linspace(0, len(training_results) / hps.n_batches, len(training_results)), \n",
    "         training_results, label='Negative Log Likelihood')\n",
    "#plt.yscale('symlog')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Bits per component')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = np.random.randint(0, hps.batch_size)\n",
    "spectrum = spectra[i:i+1, :, :]\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#37 is the spectra used for figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_rep, intermediate_zs = m.encode(spectrum)\n",
    "reconstruction = m.decode(latent_rep)\n",
    "perfect_reconstruction = m.decode(latent_rep, intermediate_zs)\n",
    "print(latent_rep.mean(), latent_rep.std())\n",
    "#print(reconstruction.mean(), reconstruction.std())\n",
    "print(np.mean((spectrum - reconstruction)), np.mean((spectrum - perfect_reconstruction)))\n",
    "print(np.mean((spectrum - reconstruction)**2), np.mean((spectrum - perfect_reconstruction)**2))\n",
    "print(np.mean((spectrum - reconstruction)**4), np.mean((spectrum - perfect_reconstruction)**4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2, 2))\n",
    "plt.plot(latent_rep.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = (0, hps.n_bins)\n",
    "window = (int(hps.n_bins*.4), int(hps.n_bins*.6)) \n",
    "#window = (12000, 14000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0, 30000, 2**12) # in angstrom\n",
    "lambdas = lambdas[window[0]:window[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(lambdas, np.squeeze(reconstruction)[window[0]:window[1]], label='x')\n",
    "plt.plot(lambdas, np.squeeze(spectrum)[window[0]:window[1]], label='d(z)')\n",
    "plt.xlabel('Wavelength $[\\AA]$')\n",
    "plt.ylabel('Normalized flux')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(lambdas, np.squeeze(reconstruction - spectrum)[window[0]:window[1]])\n",
    "plt.xlabel('Wavelength $[\\AA]$')\n",
    "plt.ylabel('d(z) - x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_shapes = []\n",
    "for intermediate_z in intermediate_zs:\n",
    "    intermediate_shapes.append(intermediate_z.shape)\n",
    "intermediate_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = []\n",
    "for intermediate_zs_used in range(len(intermediate_shapes)+1):\n",
    "    new_intermediate_zs = []\n",
    "    for i in range(len(intermediate_shapes)):\n",
    "        #print(i, intermediate_zs_used, i < len(intermediate_shapes) - intermediate_zs_used)\n",
    "        if i < len(intermediate_shapes) - intermediate_zs_used:\n",
    "            sampled_z = np.random.normal(0, 1, intermediate_shapes[i])\n",
    "            new_intermediate_zs.append(sampled_z)\n",
    "        else:\n",
    "            new_intermediate_zs.append(intermediate_zs[i])\n",
    "    reconstructions.append(m.decode(latent_rep, new_intermediate_zs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "for i in [0, 1, 2]:\n",
    "    print(np.mean((reconstructions[i] - spectrum)**2))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(lambdas, np.squeeze(reconstructions[i])[window[0]:window[1]], \n",
    "             label='d($\\widetilde{{h}}_{})$'.format(i), alpha=.75)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(lambdas, np.squeeze(reconstructions[i] - spectrum)[window[0]:window[1]], \n",
    "             label='d($\\widetilde{{h}}_{}) - x$'.format(i), alpha=.75)\n",
    "    \n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(lambdas, np.squeeze(spectrum)[window[0]:window[1]], label='x')\n",
    "plt.xlabel('Wavelength $[\\AA]$')\n",
    "plt.ylabel('Normalized flux')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.axhline(0, color='k')\n",
    "plt.xlabel('Wavelength $[\\AA]$')\n",
    "plt.ylabel('d($\\widetilde{h}_i$) - x')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1s = []\n",
    "l2s = []\n",
    "l4s = []\n",
    "for r in reconstructions:\n",
    "    l1s.append(np.mean(np.abs(r - spectrum)))\n",
    "    l2s.append(np.mean((r - spectrum)**2)**.5)\n",
    "    l4s.append(np.mean((r - spectrum)**4)**.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(l1s, label='Mean Absolute Error')\n",
    "plt.plot(l2s, label='Root Mean Squared Error')\n",
    "plt.plot(l4s, label='Root Mean Quartic Error')\n",
    "\n",
    "plt.xlabel('Number of Intermediate Representations used from true $h$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# latent variable behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_rep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_reps = np.empty([hps.n_batches, hps.batch_size, latent_rep.shape[1], latent_rep.shape[2]])\n",
    "initialize_stream()\n",
    "with tqdm(total=hps.n_batches) as pbar:\n",
    "    for i in range(hps.n_batches):\n",
    "        data = sess.run(input_stream)\n",
    "        latent_reps[i], _ = m.encode(data)\n",
    "        pbar.set_postfix(mean=latent_reps.mean(), std=latent_reps.std())\n",
    "        pbar.update()\n",
    "\n",
    "latent_reps = latent_reps.reshape(hps.n_data, latent_rep.shape[1] * latent_rep.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latent_reps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(means - stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1, 1))\n",
    "means = latent_reps.mean(axis=0)\n",
    "stds =  latent_reps.std(axis=0)\n",
    "\n",
    "plt.plot(means)\n",
    "plt.fill_between(range(len(means)), means - stds, means + stds, alpha=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latent_reps.mean(axis=0).round(3))\n",
    "print(latent_reps.std(axis=0).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "figure = corner.corner(latent_reps)\n",
    "\n",
    "axes = np.array(figure.axes).reshape((latent_reps.shape[1], latent_reps.shape[1]))\n",
    "for yi in range(latent_reps.shape[1]):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi]\n",
    "        ax.axvline(0, color=\"g\")\n",
    "        ax.axhline(0, color=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate random realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 2))\n",
    "for _ in range(5):\n",
    "    sampled_latent_rep = np.random.normal(size=latent_rep.shape)\n",
    "    #plt.plot(sampled_latent_rep.ravel())\n",
    "    plt.plot(np.squeeze(m.decode(sampled_latent_rep)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_spectra, test_labels = toy_data.generate_spectra(n_test, hps.n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zs, test_intermediate_zs = m.encode(test_spectra[:, :, np.newaxis])\n",
    "test_reconstructions = m.decode(test_zs)\n",
    "print(np.mean(np.abs(test_spectra - test_reconstructions.squeeze())))\n",
    "print(np.mean((test_spectra - test_reconstructions.squeeze())**2)**.5)\n",
    "print(np.mean((test_spectra - test_reconstructions.squeeze())**4)**.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_intermediate_shapes = []\n",
    "for tmp in test_intermediate_zs:\n",
    "    test_intermediate_shapes.append(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reconstructions = []\n",
    "for intermediate_zs_used in range(len(test_intermediate_shapes)+1):\n",
    "    new_intermediate_zs = []\n",
    "    for i in range(len(test_intermediate_shapes)):\n",
    "        if i < len(test_intermediate_shapes) - intermediate_zs_used:\n",
    "            sampled_z = np.random.normal(0, 1, test_intermediate_shapes[i])\n",
    "            new_intermediate_zs.append(sampled_z)\n",
    "        else:\n",
    "            new_intermediate_zs.append(test_intermediate_zs[i])\n",
    "    test_reconstructions.append(m.decode(test_zs, new_intermediate_zs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_l1s = []\n",
    "test_l2s = []\n",
    "test_l4s = []\n",
    "for r in test_reconstructions:\n",
    "    test_l1s.append(np.mean(np.abs(r.squeeze() - test_spectra)))\n",
    "    test_l2s.append(np.mean((r.squeeze() - test_spectra)**2)**.5)\n",
    "    test_l4s.append(np.mean((r.squeeze() - test_spectra)**4)**.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(test_l1s, label='Mean Absolute Error')\n",
    "plt.plot(test_l2s, label='Root Mean Squared Error')\n",
    "plt.plot(test_l4s, label='Root Mean Quartic Error')\n",
    "\n",
    "plt.xlabel('Number of Intermediate Representations used from true $h$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "model_filename = 'models/model-{}'.format(datetime.now().strftime('%y%m%d-%H%M%S'))\n",
    "print(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.restore('models/model-200304-081901')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
